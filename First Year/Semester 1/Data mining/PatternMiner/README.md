# Comparing FP-Growth and Apriori Algorithms for Frequent Itemset Mining

## Overview

This project evaluates and compares the **FP-Growth** and **Apriori** algorithms. Both
algorithms identify frequent itemsets from transactional data. This comparison includes their execution time
for various datasets.

### Objectives

1. Implement FP-Growth and Apriori.
2. Ensure both algorithms produce identical results.
3. Compare their performance (execution time).

### Requirements

1. Python 3.x
2. Libraries
    - `pandas`
    - `time`
    - `numpy`

Install the required libraries using:

```bash
pip/conda install time pandas numpy
```

### Outputs

The script outputs:

1. The input data (transaction table).
2. **Frequent itemsets** generated by both algorithms.
3. **Execution time** for both algorithms.
4. A validation message to confirm if the outputs match.

### How to Run

1. Save the files in a folder.
2. Run the script using Python:

```bash
python3 main.py
```

### Comparison of Apriori and FP-Growth Algorithms

Both **Apriori** and **FP-Growth** are popular algorithms for discovering **frequent itemsets** and generating **association rules** from transactional data. While they produce identical results in terms of **Support**, **Confidence**, and **Lift**, they differ in their approach and performance:

- **Apriori**: A straightforward, iterative algorithm that scans the dataset multiple times. It works well for smaller
  datasets but can be slower as data size increases.

- **FP-Growth**: A more efficient algorithm that uses a compact tree structure (FP-Tree) to reduce dataset scans, making
  it faster and more scalable for larger datasets.

Both methods successfully identify meaningful relationships between items, helping uncover strong associations and
patterns that can be used for tasks like **market basket analysis**, **recommendation systems**, and **product bundling**.

---

## Key Metrics in Frequent Itemset Mining

### **1. Support**
**Definition:**  
**Support** measures the frequency of an itemset occurring in the dataset.

**Formula:**  
Support(A) = (Transactions containing A) / (Total number of transactions)

**Example:**  
If we have 1,000 transactions and "Milk" appears in 200 of them:  

Support(Milk) = 200 / 1000 = 0.2

**Why It Matters:**  
- **Higher support** means an item or itemset is frequent and important.
- **Minimum Support Threshold:** Filters out rare items that are unlikely to provide valuable insights.

---

### **2. Confidence**
**Definition:**  
**Confidence** measures how often items in `B` appear in transactions that contain `A`. It evaluates the reliability of a rule **A ⇒ B**.

**Formula:**  
Confidence(A ⇒ B) = Support(A ∪ B) / Support(A)

**Example:**  
Suppose out of 200 transactions with "Milk," 150 also have "Bread":  

Confidence(Milk ⇒ Bread) = 150 / 200 = 0.75

**Why It Matters:**  
- **Higher confidence** means the rule is more reliable.
- If confidence is **low**, the association between `A` and `B` may be weak or coincidental.

---

### **3. Lift** *(Optional for additional insights)*
**Definition:**  
**Lift** measures the strength of a rule compared to the expected occurrence of `B`, assuming `A` and `B` are independent.

**Formula:**  
Lift(A ⇒ B) = Confidence(A ⇒ B) / Support(B)

**Example:**  
If Support(Bread) = 0.3 and Confidence(Milk ⇒ Bread) = 0.75:  

Lift(Milk ⇒ Bread) = 0.75 / 0.3 = 2.5

**Why It Matters:**  
- A **Lift > 1** indicates a strong positive association.
- **Lift = 1** means `A` and `B` are independent.
- A **Lift < 1** suggests a negative correlation.

---

### Future Improvements

- Add support for filtering by **product categories** or **price ranges**.  
- Include time-based analysis (e.g., **monthly or seasonal trends**).  
- Enable filtering by **specific customers** or **transaction size**.
- Implement other algorithms to test (like Eclat and RARM)